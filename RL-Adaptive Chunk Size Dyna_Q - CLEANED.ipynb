{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming import StreamingEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set alpha value\n",
    "alpha = 0.5\n",
    "\n",
    "#set epsilon to a small value\n",
    "epsilon = .05\n",
    "\n",
    "discount = .9\n",
    "\n",
    "env = StreamingEnvironment()  # create an instance of the environemnt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Q as Q[buffer][throughput(we only use 1-4)][future_throughput(we only use 1-4)][value of action(we only use 1-4)]\n",
    "Q = np.zeros((11,5,5,5))\n",
    "for i in range(11):\n",
    "    \n",
    "    Q[i] = i\n",
    "    for j in range(5):\n",
    "        \n",
    "        Q[i][j] = j\n",
    "        for k in range(5):\n",
    "            \n",
    "            Q[i][j][k] = k\n",
    "            for val in range(5):\n",
    "                Q[i][j][k][val] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = input(\"run the episodes with DEBUG? (L,M,H,n) (L - low)(M - medium) (H - high) (n - no)\")\n",
    "eps = input('How many episodes?')\n",
    "for _ in range(int(eps)):\n",
    "    \n",
    "    done = False\n",
    "    if DEBUG == 'L' or DEBUG =='M' or DEBUG == 'H' or DEBUG =='n':\n",
    "        print(\"===============================NEW EPISODE=================================\")\n",
    "    #step = env.reset()\n",
    "    cumulative_reward = 0\n",
    "    State = np.array(env.reset().observation, dtype=int)\n",
    "    observed_states = []\n",
    "    observed_actions = []\n",
    "    while not done:       \n",
    "        # current S\n",
    "        observed_states.append(np.array(State))\n",
    "        print(\"State is: \", State.tolist())\n",
    "\n",
    "        #choose A from S\n",
    "        if DEBUG == 'M' or DEBUG == 'H':\n",
    "            print(\"the current Q in state S: \", State.tolist(), \" Q: \", Q[State[0],State[1],State[2]].flatten())\n",
    "       # actionset = env.valid_actions(State)\n",
    "        if env.Last_packet != 0:\n",
    "            action = 0\n",
    "            print(\"waiting on last packet: \", env.Last_action)\n",
    "        else:\n",
    "            if np.random.rand() < 1 - epsilon:\n",
    "\n",
    "                #print(\"the value of the np.argmax is: \", np.argmax(Q[State[0],State[1],State[2]].flatten()))\n",
    "                #print(\"the value of the np.amax is : \", np.amax(Q[State[0],State[1],State[2]].flatten()))\n",
    "                choices = np.argwhere(Q[State[0],State[1],State[2]].flatten() == np.amax(Q[State[0],State[1],State[2]].flatten()))\n",
    "                if DEBUG == 'L' or DEBUG =='M' or DEBUG =='H':\n",
    "                    print(\"choices are: \", choices.flatten())\n",
    "                action = np.random.choice(choices.flatten())\n",
    "\n",
    "            else:\n",
    "                action = np.random.choice([1,2,3,4])\n",
    "       \n",
    "        #if DEBUG =='M' or DEBUG =='H':\n",
    "        #    print(\"Executing action:\",ACTION[action])\n",
    "        print(\"Executing action: \", action)\n",
    "        observed_actions.append(action)\n",
    "\n",
    "        #take action A and observe next state S' and reward R\n",
    "        step = env.step(action)\n",
    "        state_prime = np.array(step.observation, dtype=int)\n",
    "        if DEBUG == 'L' or DEBUG =='M' or DEBUG =='H':\n",
    "            print(\"S is: \", State.tolist(),\" executing action: \",action, \"and S' is: \",state_prime.tolist())\n",
    "\n",
    "        reward = step.reward\n",
    "        cumulative_reward += reward\n",
    "        \n",
    "        if DEBUG == 'M' or DEBUG == 'L':\n",
    "            print(\"Reward was: \", reward)\n",
    "        \n",
    "        if DEBUG == 'H':\n",
    "            print(\"** Updating Q values now**\")\n",
    "            print(\"** Current value of Q for action ,\",action, \" in this state \", State.tolist(), \" is:  \", Q[State[0],State[1],State[2],[action]])\n",
    "            print(\"** reward was: \", reward)\n",
    "            print(\"** argmax of Q for S' was: \",np.max(Q[state_prime[0],state_prime[1],state_prime[2]]))\n",
    "            print(\"** Q for the end of formula was: \", Q[State[0],State[1],State[2],[action]])\n",
    "            print(\"** S' is: \",state_prime.tolist())\n",
    "        \n",
    "        #update our Q\n",
    "        Q[State[0],State[1],State[2],[action]] += alpha * (reward + discount*np.max(Q[state_prime[0],state_prime[1],state_prime[2]]) - Q[State[0],State[1],State[2],[action]])\n",
    "        if DEBUG == 'H':\n",
    "             print(\"** After update, the value of Q for action ,\", action , \" in this state \", State.tolist(), \" is:  \", Q[State[0],State[1],State[2],[action]])\n",
    "        State = state_prime\n",
    "        \n",
    "        #looping min(length of observed_states or n) times for the planning stage\n",
    "        if DEBUG == 'H':\n",
    "            print(\"- - Entering the planning stage\")\n",
    "            print(\"- - length of the observed states list \",len(observed_states))\n",
    "            print(\"- - observed_states contains, \", observed_states)\n",
    "        for n in range(min(len(observed_states),25)):\n",
    "            \n",
    "            #pick previously visited state S\n",
    "            randChoice = np.random.randint(low=0,high=len(observed_states))\n",
    "            randS = tuple(observed_states[randChoice])\n",
    "            if  DEBUG == 'H':\n",
    "                print(\"- - chose random state :\",randS)\n",
    "            #take action A (a random valid action for the state)\n",
    "            randA = np.random.choice([1,2,3,4])\n",
    "            if  DEBUG == 'H':\n",
    "                print(\"- - chose a random action\",randA)\n",
    "            \n",
    "            old_state = list(randS)\n",
    "            #query the model for S' and R\n",
    "            next_state, model_reward = env.sample_new_state(old_state,randA)\n",
    "            if  DEBUG == 'H':\n",
    "                print(\"- - model gave sample s' of \", next_state.tolist(), \" and reward of \", model_reward)\n",
    "            #update Q\n",
    "            if DEBUG == 'H':\n",
    "                print(\"- - ** initial value of Q for action \", randA,\" is \", Q[randS[0],randS[1],randS[2],[randA]], \" argmax of Q(S',a) is \",np.max(Q[next_state[0],next_state[1],next_state[2]]) )\n",
    "            Q[randS[0],randS[1],randS[2],[randA]] += alpha * (model_reward + discount*np.max(Q[next_state[0],next_state[1],next_state[2]]) - Q[randS[0],randS[1],randS[2],[randA]])\n",
    "            if DEBUG == 'H':\n",
    "                print(\"- - ** adjusted value of Q is \",Q[randS[0],randS[1],randS[2],[randA]] )\n",
    "\n",
    "        #print(State[0])\n",
    "        #checking for the end of the episode        \n",
    "        if step.is_last():\n",
    "            print(\"Buffer Empty!!\")\n",
    "            observed_states.append(state_prime)\n",
    "            done = True\n",
    "\n",
    "        #elif step.reward == 1:\n",
    "            #print(\"Maximized throughput usage!!\")\n",
    "            #print(\"reward so far is:\", cumulative_reward)\n",
    "            \n",
    "        elif State[0] == [10]:\n",
    "            print(\"Finished one buffer! Quitting now\")\n",
    "            observed_states.append(state_prime)\n",
    "            done = True\n",
    "            \n",
    "    #episode finished PRINT THE PATH TAKEN\n",
    "    if DEBUG == 'L' or DEBUG == 'M' or DEBUG == 'H':\n",
    "        for i in range(0,len(observed_states)-1):\n",
    "            #S,A = episode[i]\n",
    "            print(\"The path was state \", observed_states[i].tolist(),\" and action \",observed_actions[i])\n",
    "            \n",
    "        print(\"Final State was \", observed_states[-1].tolist())\n",
    "\n",
    "    print(\"episode reward was: \", cumulative_reward)\n",
    "    episode_reward.append(cumulative_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(episode_reward)\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ccf745",
   "metadata": {},
   "outputs": [],
   "source": [
    "throughput_list = []\n",
    "buffer_list = []\n",
    "for i in range(len(observed_states)):\n",
    "    #print(observed_states[i][1])\n",
    "    throughput_list.append(int(observed_states[i][1]))\n",
    "    buffer_list.append(int(observed_states[i][0]))\n",
    "print(throughput_list)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(throughput_list, label='throughput')\n",
    "plt.plot(buffer_list, label='buffer occupancy')\n",
    "plt.plot(observed_actions, label='packet size')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Units\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.ravel(Q)\n",
    "\n",
    "fig = plt.figure(figsize=(10,9))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "_states = []\n",
    "for a in range(275):\n",
    "    _states.append(a)\n",
    "    \n",
    "#x = state_array\n",
    "#print(state_array)\n",
    "x = _states\n",
    "y = [0,1,2,3,4]\n",
    "X, Y = np.meshgrid(x,y)\n",
    "#_z = np.array(z)\n",
    "_z = z.reshape((5,275))\n",
    "\n",
    "#print(Q[1])\n",
    "\n",
    "ax.plot_surface(X,Y,_z,cmap=cm.coolwarm, antialiased=True, alpha=0.75, rstride=10, cstride=2)\n",
    "plt.xlabel('States')\n",
    "plt.ylabel('Actions')\n",
    "ax.set_zlabel('Q(s,a)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TfAgentsEnv]",
   "language": "python",
   "name": "conda-env-TfAgentsEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
